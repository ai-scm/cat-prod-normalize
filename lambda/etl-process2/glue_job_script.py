import sys
import boto3
from datetime import datetime, date
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, to_timestamp, when, isnan, isnull
from pyspark.sql.types import IntegerType, DoubleType, DateType, TimestampType
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

# Argumentos del job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'input_bucket',
    'input_prefix', 
    'output_bucket',
    'output_prefix'
])

# Inicializar contextos
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Configuraci√≥n
INPUT_BUCKET = args['input_bucket']
INPUT_PREFIX = args['input_prefix']  # reports/etl-process1/
OUTPUT_BUCKET = args['output_bucket']
OUTPUT_PREFIX = args['output_prefix']  # reports/etl-process2/

def main():
    """
    Funci√≥n principal del job de Glue
    """
    print(f"üöÄ INICIANDO ETL-2 PROCESS - PYSPARK + TYPE CONVERSION")
    print(f"   ‚ö° Motor: PySpark distribuido con conversi√≥n de tipos")
    print(f"   üì• Input: s3://{INPUT_BUCKET}/{INPUT_PREFIX}")
    print(f"   üì§ Output: s3://{OUTPUT_BUCKET}/{OUTPUT_PREFIX}")
    print(f"   üéØ Objetivo: Conversi√≥n CSV ‚Üí Parquet con tipos correctos")
    print(f"   üîß Soluci√≥n: Problema de 'todo como string' resuelto")
    
    try:
        # 1. Leer CSV m√°s reciente de ETL-1
        input_path = f"s3://{INPUT_BUCKET}/{INPUT_PREFIX}"
        print(f"üìñ Leyendo CSV desde: {input_path}")
        
        # Leer usando Spark para mejor rendimiento
        df_spark = spark.read.option("header", "true").csv(input_path)
        
        total_records = df_spark.count()
        total_columns = len(df_spark.columns)
        print(f"üìä Registros le√≠dos: {total_records}")
        print(f"üìã Columnas encontradas: {total_columns}")
        print(f"üìù Nombres de columnas: {df_spark.columns}")
        
        # Mostrar esquema original del CSV
        print(f"\nüìã ESQUEMA ORIGINAL DEL CSV:")
        df_spark.printSchema()
        
        # 2. Procesar datos usando solo PySpark (sin transformaciones)
        # Mantiene exactamente el mismo formato que viene del CSV
        df_processed = process_data(df_spark)
        
        # 3. Escribir como Parquet √∫nico (overwrite)
        output_path = f"s3://{OUTPUT_BUCKET}/{OUTPUT_PREFIX}data.parquet"
        print(f"\nüíæ Escribiendo Parquet a: {output_path}")
        print(f"   üéØ Modo: Archivo √∫nico (overwrite)")
        print(f"   üìä Registros a escribir: {df_processed.count()}")
        
        # Escribir como archivo √∫nico (coalesce a 1 partici√≥n)
        df_processed.coalesce(1).write.mode("overwrite").parquet(output_path)
        
        # 4. Verificaci√≥n
        verify_output(output_path)
        
        print(f"\nüéâ ETL-2 COMPLETADO EXITOSAMENTE")
        print(f"   ‚úÖ Conversi√≥n CSV ‚Üí Parquet con tipos correctos")
        print(f"   ‚úÖ Problema de 'todo string' resuelto")
        print(f"   ‚úÖ {total_records} registros procesados")
        print(f"   ‚ö° Motor utilizado: PySpark distribuido + Type Conversion")
        
    except Exception as e:
        print(f"‚ùå ERROR EN ETL-2: {str(e)}")
        import traceback
        print(f"üîç TRACEBACK COMPLETO:")
        print(traceback.format_exc())
        raise

def process_data(df_spark):
    """
    Procesa los datos usando PySpark + convierte tipos apropiados
    Corrige el problema de que todo se detecte como string
    
    Args:
        df_spark: DataFrame de Spark con los datos CSV
        
    Returns:
        DataFrame de Spark con tipos correctos
    """
    print("üîÑ Procesando datos - PYSPARK + CONVERSI√ìN DE TIPOS...")
    print(f"   ‚ö° Usando motor distribuido de Spark")
    print(f"   üéØ Objetivo: CSV ‚Üí Parquet con tipos correctos")
    print(f"   ÔøΩ Solucionando problema: TODO detectado como string")
    
    # Mostrar informaci√≥n b√°sica del DataFrame
    total_records = df_spark.count()
    total_columns = len(df_spark.columns)
    
    print(f"\nüìä INFORMACI√ìN DEL DATASET:")
    print(f"   üìà Registros: {total_records}")
    print(f"   ÔøΩ Columnas: {total_columns}")
    print(f"   üìù Nombres: {df_spark.columns}")
    
    # Mostrar esquema
    print(f"\nüìã ESQUEMA ORIGINAL (CSV - todo string):")
    df_spark.printSchema()
    
    # üîß CONVERSI√ìN DE TIPOS ESPEC√çFICA
    print(f"\nüîß APLICANDO CONVERSI√ìN DE TIPOS...")
    
    df_processed = df_spark
    
    # Definir conversiones por columna esperada (case-insensitive)
    # COLUMNAS REALES DEL DATASET CON TIPOS ESPECIFICADOS:
    # usuario_id->String, nombre->String, gerencia->String, ciudad->String, 
    # fecha_primera_conversacion->Date, numero_conversaciones->int, 
    # conversacion_completa->String, feedback_total->String, 
    # numero_feedback->int, pregunta_conversacion->String, 
    # feedback->String, respuesta_feedback->String
    type_conversions = {
        # Fechas (formato DD/MM/YYYY detectado en el dataset real)
        'fecha_primera_conversacion': 'date',
        
        # Enteros
        'numero_conversaciones': 'integer',
        'numero_feedback': 'integer',
        
        # Strings expl√≠citos (aunque por defecto ya son string, los definimos para claridad)
        'usuario_id': 'string',
        'nombre': 'string', 
        'gerencia': 'string',
        'ciudad': 'string',
        'conversacion_completa': 'string',
        'feedback_total': 'string',
        'pregunta_conversacion': 'string',
        'feedback': 'string',
        'respuesta_feedback': 'string'
    }
    
    # Aplicar conversiones seg√∫n las columnas presentes
    for column in df_processed.columns:
        if column.lower() in type_conversions:
            conversion_type = type_conversions[column.lower()]
            
            print(f"   üîÑ Convirtiendo '{column}' ‚Üí {conversion_type}")
            
            try:
                if conversion_type == 'date':
                    # Intentar varios formatos de fecha comunes
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(
                            # Intentar formato YYYY-MM-DD primero
                            when(col(column).rlike(r'^\d{4}-\d{2}-\d{2}$'), to_date(col(column), 'yyyy-MM-dd'))
                            # Luego DD/MM/YYYY
                            .when(col(column).rlike(r'^\d{2}/\d{2}/\d{4}$'), to_date(col(column), 'dd/MM/yyyy'))
                            # Luego MM/DD/YYYY
                            .when(col(column).rlike(r'^\d{1,2}/\d{1,2}/\d{4}$'), to_date(col(column), 'MM/dd/yyyy'))
                            # Si no coincide, intentar auto-detect
                            .otherwise(to_date(col(column)))
                        )
                    )
                    
                elif conversion_type == 'timestamp':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(to_timestamp(col(column)))
                    )
                    
                elif conversion_type == 'integer':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == "") | (col(column) == "0"), None)
                        .otherwise(col(column).cast(IntegerType()))
                    )
                    
                elif conversion_type == 'double':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(col(column).cast(DoubleType()))
                    )
                    
                elif conversion_type == 'string':
                    # Para strings, solo limpiamos valores nulos
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull(), None)
                        .otherwise(col(column).cast("string"))
                    )
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error convirtiendo {column}: {str(e)} - manteniendo como string")
                continue
        else:
            print(f"   üìã Manteniendo '{column}' como string (no en mapping)")
    
    # Mostrar esquema final con tipos correctos
    print(f"\n‚úÖ ESQUEMA FINAL (con tipos correctos):")
    df_processed.printSchema()
    
    # Mostrar muestra de datos procesados
    print(f"\nüìù MUESTRA DE DATOS PROCESADOS (primeras 3 filas):")
    df_processed.show(3, truncate=False)
    
    # Estad√≠sticas b√°sicas por columna
    print(f"\nüìä ESTAD√çSTICAS POST-CONVERSI√ìN:")
    for column in df_processed.columns:
        non_null_count = df_processed.filter(df_processed[column].isNotNull()).count()
        null_count = total_records - non_null_count
        column_type = dict(df_processed.dtypes)[column]
        print(f"   üìã {column} ({column_type}): {non_null_count} v√°lidos, {null_count} nulos")
    
    print(f"\nüéâ Procesamiento PySpark + Conversi√≥n de Tipos completado")
    print(f"   ‚úÖ Problema de 'todo como string' resuelto")
    print(f"   ‚úÖ Tipos apropiados aplicados para analytics")
    
    return df_processed

def verify_output(output_path):
    """
    Verifica que el archivo Parquet se escribi√≥ correctamente
    y muestra detalles de los datos finales
    """
    print("üîç Verificando output...")
    
    try:
        # Leer el archivo reci√©n escrito
        df_verify = spark.read.parquet(output_path)
        record_count = df_verify.count()
        column_count = len(df_verify.columns)
        
        print(f"\n‚úÖ VERIFICACI√ìN EXITOSA:")
        print(f"   üìä Registros en Parquet: {record_count}")
        print(f"   üìã Columnas en Parquet: {column_count}")
        print(f"   üìù Nombres de columnas: {df_verify.columns}")
        
        # Mostrar esquema final del Parquet
        print(f"\nüìã ESQUEMA FINAL DEL PARQUET (con tipos correctos):")
        df_verify.printSchema()
        
        # Verificar tipos espec√≠ficos
        print(f"\nüîß VERIFICACI√ìN DE TIPOS:")
        for column_name, column_type in df_verify.dtypes:
            print(f"   üìã {column_name}: {column_type}")
        
        # Mostrar muestra de los datos finales
        print(f"\nüìù MUESTRA DE DATOS FINALES (primeras 3 filas):")
        df_verify.show(3, truncate=False)
        
        # Informaci√≥n adicional del archivo
        print(f"\nüìÅ INFORMACI√ìN DEL ARCHIVO:")
        print(f"   üìç Ubicaci√≥n: {output_path}")
        print(f"   üóÉÔ∏è Formato: Parquet (columnar) con tipos correctos")
        print(f"   üì¶ Compresi√≥n: Autom√°tica")
        print(f"   üîÑ Modo escritura: Overwrite (archivo √∫nico)")
        print(f"   ‚úÖ Analytics-ready: Tipos apropiados para consultas")
        
    except Exception as e:
        print(f"‚ùå Error en verificaci√≥n: {str(e)}")
        import traceback
        print(f"üîç TRACEBACK:")
        print(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
    job.commit()
