import sys
import boto3
from datetime import datetime, date
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, to_timestamp, when, isnan, isnull
from pyspark.sql.types import IntegerType, DoubleType, DateType, TimestampType
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

# Argumentos del job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'input_bucket',
    'input_prefix', 
    'output_bucket',
    'output_prefix'
])

# Inicializar contextos
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ConfiguraciÃ³n
INPUT_BUCKET = args['input_bucket']
INPUT_PREFIX = args['input_prefix']  # reports/etl-process1/
OUTPUT_BUCKET = args['output_bucket']
OUTPUT_PREFIX = args['output_prefix']  # reports/etl-process2/

def main():
    """
    FunciÃ³n principal del job de Glue
    """
    print(f"ğŸš€ INICIANDO ETL-2 PROCESS - PYSPARK + TYPE CONVERSION")
    print(f"   âš¡ Motor: PySpark distribuido con conversiÃ³n de tipos")
    print(f"   ğŸ“¥ Input: s3://{INPUT_BUCKET}/{INPUT_PREFIX}")
    print(f"   ğŸ“¤ Output: s3://{OUTPUT_BUCKET}/{OUTPUT_PREFIX}")
    print(f"   ğŸ¯ Objetivo: ConversiÃ³n CSV â†’ Parquet con tipos correctos")
    print(f"   ğŸ”§ SoluciÃ³n: Problema de 'todo como string' resuelto")
    
    try:
        # 1. Leer CSV mÃ¡s reciente de ETL-1
        input_path = f"s3://{INPUT_BUCKET}/{INPUT_PREFIX}"
        print(f"ğŸ“– Leyendo CSV desde: {input_path}")
        
        # Leer usando Spark para mejor rendimiento
        df_spark = spark.read.option("header", "true").csv(input_path)
        
        total_records = df_spark.count()
        total_columns = len(df_spark.columns)
        print(f"ğŸ“Š Registros leÃ­dos: {total_records}")
        print(f"ğŸ“‹ Columnas encontradas: {total_columns}")
        print(f"ğŸ“ Nombres de columnas: {df_spark.columns}")
        
        # Mostrar esquema original del CSV
        print(f"\nğŸ“‹ ESQUEMA ORIGINAL DEL CSV:")
        df_spark.printSchema()
        
        # 2. Procesar datos usando solo PySpark (sin transformaciones)
        # Mantiene exactamente el mismo formato que viene del CSV
        df_processed = process_data(df_spark)
        
        # 3. Escribir como Parquet Ãºnico (overwrite)
        output_path = f"s3://{OUTPUT_BUCKET}/{OUTPUT_PREFIX}data.parquet"
        print(f"\nğŸ’¾ Escribiendo Parquet a: {output_path}")
        print(f"   ğŸ¯ Modo: Archivo Ãºnico (overwrite)")
        print(f"   ğŸ“Š Registros a escribir: {df_processed.count()}")
        
        # Escribir como archivo Ãºnico (coalesce a 1 particiÃ³n)
        df_processed.coalesce(1).write.mode("overwrite").parquet(output_path)
        
        # 4. VerificaciÃ³n
        verify_output(output_path)
        
        print(f"\nğŸ‰ ETL-2 COMPLETADO EXITOSAMENTE")
        print(f"   âœ… ConversiÃ³n CSV â†’ Parquet con tipos correctos")
        print(f"   âœ… Problema de 'todo string' resuelto")
        print(f"   âœ… {total_records} registros procesados")
        print(f"   âš¡ Motor utilizado: PySpark distribuido + Type Conversion")
        
    except Exception as e:
        print(f"âŒ ERROR EN ETL-2: {str(e)}")
        import traceback
        print(f"ğŸ” TRACEBACK COMPLETO:")
        print(traceback.format_exc())
        raise

def process_data(df_spark):
    """
    Procesa los datos usando PySpark + convierte tipos apropiados
    Corrige el problema de que todo se detecte como string
    
    Args:
        df_spark: DataFrame de Spark con los datos CSV
        
    Returns:
        DataFrame de Spark con tipos correctos
    """
    print("ğŸ”„ Procesando datos - PYSPARK + CONVERSIÃ“N DE TIPOS...")
    print(f"   âš¡ Usando motor distribuido de Spark")
    print(f"   ğŸ¯ Objetivo: CSV â†’ Parquet con tipos correctos")
    print(f"   ï¿½ Solucionando problema: TODO detectado como string")
    
    # Mostrar informaciÃ³n bÃ¡sica del DataFrame
    total_records = df_spark.count()
    total_columns = len(df_spark.columns)
    
    print(f"\nğŸ“Š INFORMACIÃ“N DEL DATASET:")
    print(f"   ğŸ“ˆ Registros: {total_records}")
    print(f"   ï¿½ Columnas: {total_columns}")
    print(f"   ğŸ“ Nombres: {df_spark.columns}")
    
    # Mostrar esquema
    print(f"\nğŸ“‹ ESQUEMA ORIGINAL (CSV - todo string):")
    df_spark.printSchema()
    
    # ğŸ”§ CONVERSIÃ“N DE TIPOS ESPECÃFICA
    print(f"\nğŸ”§ APLICANDO CONVERSIÃ“N DE TIPOS...")
    
    df_processed = df_spark
    
    # Definir conversiones por columna esperada (case-insensitive)
    # COLUMNAS REALES DEL DATASET CON TIPOS ESPECIFICADOS:
    # usuario_id->String, nombre->String, gerencia->String, ciudad->String, 
    # fecha_primera_conversacion->Date, numero_conversaciones->int, 
    # conversacion_completa->String, feedback_total->String, 
    # numero_feedback->int, pregunta_conversacion->String, 
    # feedback->String, respuesta_feedback->String
    type_conversions = {
        # Fechas (formato DD/MM/YYYY detectado en el dataset real)
        'fecha_primera_conversacion': 'date',
        
        # Enteros
        'numero_conversaciones': 'integer',
        'numero_feedback': 'integer',
        
        # Strings explÃ­citos (aunque por defecto ya son string, los definimos para claridad)
        'usuario_id': 'string',
        'nombre': 'string', 
        'gerencia': 'string',
        'ciudad': 'string',
        'conversacion_completa': 'string',
        'feedback_total': 'string',
        'pregunta_conversacion': 'string',
        'feedback': 'string',
        'respuesta_feedback': 'string'
    }
    
    # Aplicar conversiones segÃºn las columnas presentes
    for column in df_processed.columns:
        if column.lower() in type_conversions:
            conversion_type = type_conversions[column.lower()]
            
            print(f"   ğŸ”„ Convirtiendo '{column}' â†’ {conversion_type}")
            
            try:
                if conversion_type == 'date':
                    # Intentar varios formatos de fecha comunes
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(
                            # Intentar formato YYYY-MM-DD primero
                            when(col(column).rlike(r'^\d{4}-\d{2}-\d{2}$'), to_date(col(column), 'yyyy-MM-dd'))
                            # Luego DD/MM/YYYY
                            .when(col(column).rlike(r'^\d{2}/\d{2}/\d{4}$'), to_date(col(column), 'dd/MM/yyyy'))
                            # Luego MM/DD/YYYY
                            .when(col(column).rlike(r'^\d{1,2}/\d{1,2}/\d{4}$'), to_date(col(column), 'MM/dd/yyyy'))
                            # Si no coincide, intentar auto-detect
                            .otherwise(to_date(col(column)))
                        )
                    )
                    
                elif conversion_type == 'timestamp':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(to_timestamp(col(column)))
                    )
                    
                elif conversion_type == 'integer':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == "") | (col(column) == "0"), None)
                        .otherwise(col(column).cast(IntegerType()))
                    )
                    
                elif conversion_type == 'double':
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull() | (col(column) == ""), None)
                        .otherwise(col(column).cast(DoubleType()))
                    )
                    
                elif conversion_type == 'string':
                    # Para strings, solo limpiamos valores nulos
                    df_processed = df_processed.withColumn(
                        column,
                        when(col(column).isNull(), None)
                        .otherwise(col(column).cast("string"))
                    )
                    
            except Exception as e:
                print(f"   âš ï¸  Error convirtiendo {column}: {str(e)} - manteniendo como string")
                continue
        else:
            print(f"   ğŸ“‹ Manteniendo '{column}' como string (no en mapping)")
    
    # Mostrar esquema final con tipos correctos
    print(f"\nâœ… ESQUEMA FINAL (con tipos correctos):")
    df_processed.printSchema()
    
    # Mostrar muestra de datos procesados
    print(f"\nğŸ“ MUESTRA DE DATOS PROCESADOS (primeras 3 filas):")
    df_processed.show(3, truncate=False)
    
    # EstadÃ­sticas bÃ¡sicas por columna
    print(f"\nğŸ“Š ESTADÃSTICAS POST-CONVERSIÃ“N:")
    for column in df_processed.columns:
        non_null_count = df_processed.filter(df_processed[column].isNotNull()).count()
        null_count = total_records - non_null_count
        column_type = dict(df_processed.dtypes)[column]
        print(f"   ğŸ“‹ {column} ({column_type}): {non_null_count} vÃ¡lidos, {null_count} nulos")
    
    print(f"\nğŸ‰ Procesamiento PySpark + ConversiÃ³n de Tipos completado")
    print(f"   âœ… Problema de 'todo como string' resuelto")
    print(f"   âœ… Tipos apropiados aplicados para analytics")
    
    return df_processed

def verify_output(output_path):
    """
    Verifica que el archivo Parquet se escribiÃ³ correctamente
    y muestra detalles de los datos finales
    """
    print("ğŸ” Verificando output...")
    
    try:
        # Leer el archivo reciÃ©n escrito
        df_verify = spark.read.parquet(output_path)
        record_count = df_verify.count()
        column_count = len(df_verify.columns)
        
        print(f"\nâœ… VERIFICACIÃ“N EXITOSA:")
        print(f"   ğŸ“Š Registros en Parquet: {record_count}")
        print(f"   ğŸ“‹ Columnas en Parquet: {column_count}")
        print(f"   ğŸ“ Nombres de columnas: {df_verify.columns}")
        
        # Mostrar esquema final del Parquet
        print(f"\nğŸ“‹ ESQUEMA FINAL DEL PARQUET (con tipos correctos):")
        df_verify.printSchema()
        
        # Verificar tipos especÃ­ficos
        print(f"\nğŸ”§ VERIFICACIÃ“N DE TIPOS:")
        for column_name, column_type in df_verify.dtypes:
            print(f"   ğŸ“‹ {column_name}: {column_type}")
        
        # Mostrar muestra de los datos finales
        print(f"\nğŸ“ MUESTRA DE DATOS FINALES (primeras 3 filas):")
        df_verify.show(3, truncate=False)
        
        # InformaciÃ³n adicional del archivo
        print(f"\nğŸ“ INFORMACIÃ“N DEL ARCHIVO:")
        print(f"   ğŸ“ UbicaciÃ³n: {output_path}")
        print(f"   ğŸ—ƒï¸ Formato: Parquet (columnar) con tipos correctos")
        print(f"   ğŸ“¦ CompresiÃ³n: AutomÃ¡tica")
        print(f"   ğŸ”„ Modo escritura: Overwrite (archivo Ãºnico)")
        print(f"   âœ… Analytics-ready: Tipos apropiados para consultas")
        
    except Exception as e:
        print(f"âŒ Error en verificaciÃ³n: {str(e)}")
        import traceback
        print(f"ğŸ” TRACEBACK:")
        print(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
    job.commit()
